{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Comments Analysis with LangSmith Insights\n",
    "\n",
    "This example demonstrates how to use LangSmith Insights to analyze YouTube comments. We'll:\n",
    "1. Fetch recent comments from YouTube videos matching a search query\n",
    "2. Store the comments in a CSV file for reference\n",
    "3. Use LangSmith Insights to automatically cluster comments by theme and sentiment\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- A YouTube Data API key ([get one here](https://console.cloud.google.com/apis/credentials))\n",
    "- A LangSmith API key ([get one here](https://smith.langchain.com/settings))\n",
    "- Install dependencies: `pip install -r requirements.txt`\n",
    "\n",
    "Set the following environment variables:\n",
    "```bash\n",
    "export YOUTUBE_API_KEY=your-youtube-api-key\n",
    "export LANGSMITH_API_KEY=your-langsmith-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: query='LangChain', lookback=21 days, max_videos=20, max_comments=60\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_PATH = DATA_DIR / \"youtube_comments.csv\"\n",
    "\n",
    "YOUTUBE_QUERY = \"LangChain\"\n",
    "YOUTUBE_LOOKBACK_DAYS = 21\n",
    "YOUTUBE_MAX_VIDEOS = 20\n",
    "YOUTUBE_MAX_COMMENTS = 60\n",
    "YOUTUBE_SEARCH_ORDER = \"date\"\n",
    "\n",
    "print(\n",
    "    f\"Configuration: query='{YOUTUBE_QUERY}', \"\n",
    "    f\"lookback={YOUTUBE_LOOKBACK_DAYS} days, \"\n",
    "    f\"max_videos={YOUTUBE_MAX_VIDEOS}, \"\n",
    "    f\"max_comments={YOUTUBE_MAX_COMMENTS}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch YouTube Comments\n",
    "\n",
    "First, we'll search for recent videos matching our query, then fetch comments from those videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 videos\n"
     ]
    }
   ],
   "source": [
    "YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "if not YOUTUBE_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"YOUTUBE_API_KEY environment variable is required. \"\n",
    "        \"Get an API key at https://console.cloud.google.com/apis/credentials\"\n",
    "    )\n",
    "\n",
    "YOUTUBE_SEARCH_URL = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "YOUTUBE_COMMENTS_URL = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "\n",
    "# Search for videos\n",
    "published_after = (\n",
    "    datetime.now(timezone.utc) - timedelta(days=YOUTUBE_LOOKBACK_DAYS)\n",
    ").isoformat()\n",
    "\n",
    "video_params = {\n",
    "    \"q\": YOUTUBE_QUERY,\n",
    "    \"part\": \"snippet\",\n",
    "    \"type\": \"video\",\n",
    "    \"order\": YOUTUBE_SEARCH_ORDER,\n",
    "    \"publishedAfter\": published_after,\n",
    "    \"maxResults\": min(50, YOUTUBE_MAX_VIDEOS),\n",
    "    \"key\": YOUTUBE_API_KEY,\n",
    "}\n",
    "\n",
    "video_resp = requests.get(YOUTUBE_SEARCH_URL, params=video_params, timeout=30)\n",
    "video_resp.raise_for_status()\n",
    "videos = [\n",
    "    item[\"id\"][\"videoId\"]\n",
    "    for item in video_resp.json().get(\"items\", [])\n",
    "][:YOUTUBE_MAX_VIDEOS]\n",
    "\n",
    "print(f\"Found {len(videos)} videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 16 comments from 20 videos\n"
     ]
    }
   ],
   "source": [
    "# Fetch comments from videos\n",
    "comment_rows = []\n",
    "\n",
    "for video_id in videos:\n",
    "    remaining = YOUTUBE_MAX_COMMENTS - len(comment_rows)\n",
    "    if remaining <= 0:\n",
    "        break\n",
    "\n",
    "    comment_params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"videoId\": video_id,\n",
    "        \"order\": \"relevance\",\n",
    "        \"textFormat\": \"plainText\",\n",
    "        \"maxResults\": min(100, remaining),\n",
    "        \"key\": YOUTUBE_API_KEY,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        comment_resp = requests.get(\n",
    "            YOUTUBE_COMMENTS_URL, params=comment_params, timeout=30\n",
    "        )\n",
    "        comment_resp.raise_for_status()\n",
    "        items = comment_resp.json().get(\"items\", [])\n",
    "\n",
    "        for item in items[:remaining]:\n",
    "            snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            comment_text = snippet.get(\"textDisplay\", \"\").strip()\n",
    "            if comment_text:\n",
    "                comment_rows.append({\"text\": comment_text})\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"Skipping video {video_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Collected {len(comment_rows)} comments from {len(videos)} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Comments to CSV\n",
    "\n",
    "Save the comments locally for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 16 comments to data/youtube_comments.csv\n"
     ]
    }
   ],
   "source": [
    "if comment_rows:\n",
    "    with CSV_PATH.open(\"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "        writer = csv.DictWriter(fh, fieldnames=[\"text\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(comment_rows)\n",
    "    print(f\"Saved {len(comment_rows)} comments to {CSV_PATH}\")\n",
    "else:\n",
    "    print(\"No comments collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze with LangSmith Insights\n",
    "\n",
    "Now we'll send the comments to LangSmith Insights for automatic clustering and analysis. The Insights API will:\n",
    "- Identify common themes across comments\n",
    "- Group similar comments together\n",
    "- Generate summaries for each cluster\n",
    "- Provide a visual interface to explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 60 comments for analysis\n",
      "\n",
      "Example comment:\n",
      "W...\n"
     ]
    }
   ],
   "source": [
    "# Format comments for the Insights API\n",
    "# chat_histories expects a list of text data (comments, conversations, support tickets, user feedback, etc.)\n",
    "chat_histories = [row[\"text\"] for row in comment_rows]\n",
    "\n",
    "print(f\"Prepared {len(chat_histories)} comments for analysis\")\n",
    "if chat_histories:\n",
    "    print(f\"\\nExample comment:\\n{chat_histories[0][0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Insights Agent is running! This can take up to 30 minutes to complete. Once the report is completed, you'll be able to see results here: https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/projects/p/740e340f-1a24-4b61-8429-3b878b68d277?tab=4&clusterJobId=b16a81bd-d3e4-4737-b249-93929cdf8212\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "\n",
    "report = client.generate_insights(\n",
    "    chat_histories=chat_histories,\n",
    "    instructions=(\n",
    "        \"These are comments from YouTube videos about LangChain. \"\n",
    "        \"I want to understand user sentiment, common questions, \"\n",
    "        \"feature requests, and pain points related to LangChain.\"\n",
    "    ),\n",
    "    name=f\"youtube-{YOUTUBE_QUERY.lower()}-{datetime.now(timezone.utc):%Y%m%d-%H%M}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
